---
layout: post
title: "Project Management Part 1 - Cross Industry Standard Process for Data Mining"
date: 2017-08-18
comments: true
---

Hey there you beautiful ladies and gentlemen, señoras y señores, damen und herren, 女士们(們) 先生们(們 ! Hope you have a great day. I am now making a "series" of posts regarding Project Management of Data Science/Mining projects. To have some inspiration for your upcoming consulting case, internal project or your master's thesis, I will share some of the frameworks I have been applying so far. In this post, I'll introduce Cross Industry Standard Process for Data Mining (CRISP-DM). 


CRISP-DM is nothing more than a general step-wise process of how to do Data Mining. Some might think this seems silly, putting this into a framework. However, it is useful due to several resons. 
1. The framework is put up in the way you actually do data mining projects. It is quite logical that you need to gather data before analyzing, for example. 
2. It is nice to have a reference point within the team, so you can refere to steps along the way, as well as splitting tasks. In the same manner, you can just send the CRISP-DM documentation to a new arriving collegue for onboarding, instead of using hours of explaining your very custom process.
3. A framework helps you focus. It get messy rather quickly if the whole team jump back and fourth along the steps, as well as delegation gets easier.

For quite some time, CRISP-DM has been the favored methology within the industry, see the poll below made by [KDnuggets](http://www.kdnuggets.com/2014/10/crisp-dm-top-methodology-analytics-data-mining-data-science-projects.html)

![center](/figs/2017-08-19-CRISP-DM/crisp_pop.png)

Let's concretizize the concept of CRISP-DM. The framework contains six steps. On every step, we can stop the whole process, or go back, if we are not satisfied to move further down the chain. The six steps of CRISP-DM is shown in the figure below. 

![center](/figs/2017-08-19-CRISP-DM/metodologia_crisp_dm1_html_52cdbecf.png)




### 1/6: Business Understanding 

Business Understanding is the first up. Here we define the problem, the wanted outcome, constraints, make a plan and so on. This is a more "general consulting" step, where workshops and asking around are key activities. We are far away from actually doing some analytics. In academia, it is akward to use the term Business Understanding, however it is still the same job to be done when trying to make a research paper. We need to find the right problem to solve, before we start solving it with prediction models and so on. A perfect solution to something nobody wants, is not very useful.


### 2/6: Data Understanding

After the Business Understanding step, we dig into the Data Understaning step. The first thing now is to actually get our hands on the data. This is actually a own substep, that is a headace. Make sure to pay attention to this, it usually takes a lot of time and effort. For a project in a large firm, just to find the guy that can give you the data you want, can indeed be challenging. So when you first get the data, you check it out. Often the data is totally crap. You have to ask some other guys to get different data, and you anyway have to do a lot of work of understanding what the columns, rows, values really mean. This are the Describe Data and Data Exploration steps. Often you do this multiple times, and try to verify the data quality. All in all, Data Understaning must not be underrated. If your data is crap, the model will definately be crap. Avoid the situation Dilbert is in right here:


![center](/figs/2017-08-19-CRISP-DM/d8.gif)


### 3/6: Data Preparation

The Data Preparation step is basically an ETL step. We now go from the raw data, and prepare it for the modeling phase. Data Cleaning is often neccesary due to missing values, odd formatting etc. This is always the case, not the most fun part but this is crucial. When we have a nice dataset, we can start doing some feature engineering, meaning constructing data. This means constructing new variables from what we already have. Some can be as simple as converting character variables to numeric variables. Or you can extract certain metrics from time-series, like average number of transactions last month. 

It is common to use a lot of different data sources. It is thus important to integrate these. In Customer Analytics, we try to add all the data sources togheter to give the best possible picture of the customer. 


### 4/6: Modeling

Finally we are here. The promised land for Data Scientist. Even though the goal with doing a Data Science project is to increase profit, avoid catastrophies or cure cancer, most of the scientists are really doing projects because they can go crazy in the modeling step. The Data Understanding and Data Preperation steps are of ETL nature, using SQL or other alternatives, we now go into R or Python (or some other nice language). The modeling technique will be partly determined by our Business Understanding, e.g. we want to predict which customer who will buy a certain product. Here we go into more detail, selecting the spesific technique that can solve our problem. So this is where the following lines enters our project:



{% highlight r %}
library(dplyr) # for data manipulation
library(caret) # for model-building
library(DMwR) # for smote implementation
library(purrr) # for functional programming (map)
library(pROC) # for AUC calculations
{% endhighlight %}


We need to know how we will test our model. There are a lot of practises on this, but this stage is not trivial. After my experience, this depend a lot on the data, as well as the business case. Moving into the modelling phase, make sure you keep track on your parameters and different methods. In many cases you want to tune and train your model for many hours, so that you keep track on your performance is key.

Lastly we do a model assessment. Here we investigate the final performance. Depending on the model, business case and data, this will variate. 



### 5/6: Evaluation

This step is reporting step. The key outcome is a go / no-go action. We need to decide if what we have done up to now actually makes sence and can be deployed. Remember that in an analysis project, you are NOT sure you will obtain the discoveries you wish for. That is the nature of analysis. Compared to for instance web design, you kind of know what you will end up with. With analysis, the whole point is that you don't know if it's going to work.

### 6/6: Deployment

If we ever get here, it means that we found something useful! Great. Then it is time for implementing, operationalization and deploying our great model. 



## Further readings

A full Step-by-step data mining guide can be found [here](https://www.the-modeling-agency.com/crisp-dm.pdf). It can indeed be useful if the above did not quite sink in, as well as adding more details around the steps and process. 

It is common in deveoplment to use project management frameworks in different dimensions. CRISP-DM can be used together with SCRUM. I have found that to be very powerfull, and will discuss that in the next post.








